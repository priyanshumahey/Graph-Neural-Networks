{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks 1: GNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Graph Neural Networks\n",
    "Graph neural networks are examples of deep graph encoders. Graph neural networks are general frameworks to define deep neural networks on graph data. To define a deep neural network over general graphs, we need to define a new kind of deep learning architecture. \n",
    "\n",
    "Why do we not simply use mutlilayer preceptrons on a flattened adjacency matrix? That's because it would depend on the arbitrary order of nodes that we used in the adjacency matrix. This type of model would not be permutation invariant. \n",
    "\n",
    "Deep neural networks will take the entire graph as a structure and then take information such as edges, nodes, layers, etc. Then we can apply a deep learning network to study it and create outputs.\n",
    "\n",
    "With deep learning algorithms, we'll be able to solve node classification, link prediction, community detection and network similarity.\n",
    "\n",
    "\n",
    "### Basics of Deep Learning\n",
    "Supervised learning is when we are given input x, and the goal is to predict lable y. Input x can be a vector of numbers, sequences, matrices, graphs, etc. We then formulate the task  as an optimization problem. We want to optimize the parameters for the deep learning network such that the loss function results in as little loss as possible. \n",
    "\n",
    "With gradient descent, we can iteratively apply it and update weights in the opposite direction of the gradient until the gradient converges. An example of a gradient descent algorithm is the stochastic gradient descent algorithm. This picks a different minibatch containing a subset of the data and uses that as the input to prevent the gradient from requiring computing the entire dataset everytime gradient descent occurs. \n",
    "\n",
    "None-linearity is another important feature of deep learning algorithms to understand. We need to introduce non-linear functions that create relationships that are non-linear. For example, we could use ReLU and Sigmoid functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for Graphs\n",
    "One of the most important things we need to discuss first is how the graph is actually passed into the deep neural network. First, let's assume that we have an input graph $G=(V,E)$ with A as the adjacency matrix (assume binary). The matrix of it's node features are described by $X \\in \\mathbb{R}^{m \\times |V|}$.\n",
    "\n",
    "A really naive approach is to join adjacency matrix and features. Then, we would feed them into a deep neural network. There is no fixed notion of locality or sliding we can apply on a graph and graphs are permutation invariant (graph doesn't have a canonical order of the nodes).\n",
    "\n",
    "#### Permutation equivariance\n",
    "For a given node representation, consider we learn a function f that maps a graph G = (A, X) to a matrix $\\mathbb{R}^{m \\times |V|}$.. Graph has m nodes and each row is the embedding of a node. Similarly, if this property hold for any pair of otrder plan i and j, we say f is a permutation equivariant function. \n",
    "\n",
    "\n",
    "### Graph Convolutional Networks\n",
    "Read a bit more into this separately in depth as this is highly important.\n",
    "https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks 2: Design Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Neural Networks\n",
    "Convolution neural networks can give us some important insight into GNNs. Below is an example of a CNN layer with 3x3 filter.\n",
    "\n",
    "!![CNN](./Images/CNN.png)\n",
    "\n",
    "Here N(v) represents the 8 neighbor pixels of v.\n",
    "\n",
    "![GNN vs CNN](./Images/GNNvCNN.png)\n",
    "\n",
    "The formulations for CNN and GNN are shown. \n",
    "\n",
    "The key difference is that we can learn different $W_{l}^{u}$ for different neighbor u for pixel v on the image. The reason is we can pick an order for the 9 neighbors using relative position to the center pixel: {(-1,-1),(-1,0),(-1,1),...,(1,1)}.\n",
    "\n",
    "CNNs can be seen as a special GNN with fixed neighbor size and ordering. The size of a filter is pre-defined for a CNN and the advantage of GNN is it processes arbitrary graphs with different degrees for each node.\n",
    "\n",
    "CNNs are not permutation equivariant. Switching the order of pixels will leads to different outputs.\n",
    "\n",
    "Transformers are one of the most popular architectures that achieves great performance in many sequence modeling tasks. Transformer layer can be seen as a special GNN that runs on a fully-connected \"word\" graph! \n",
    "\n",
    "### General GNN Framework\n",
    "The first layer of a GNN is typically message + aggregation. There are different instatiations under this perspective. When we connect GNN layers, we stack layers sequentially and there are ways of adding skip connections. The raw input graphs are not computational graphs. There is a lot of feature augmentation and graph structure augmentation happening. \n",
    "\n",
    "How do we train a GNN? There are supervised and unsupervised objectvies. There are numerous node/edge/graph level objectives that can be focused on.\n",
    "\n",
    "![GNN Framework](./Images/GNNFramework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN Layer\n",
    "The idea of a GNN layer is that is compresses a set of vectors into a single vector. It is a two step process that involves first the message and then aggregation.\n",
    "\n",
    "1. Message Computation\n",
    "There is a specific message function:\n",
    "$m_{u}^{(l)} = MSG^{(l)} (h_{u}^{(l-1)}) $\n",
    "\n",
    "The intuition is that each node will create a message which will be sent to other nodes later. \n",
    "\n",
    "2. Message Aggregation\n",
    "Each node will aggregate the messages from node v's neighbors together. We could use aggregator functions such as sum, mean or max.\n",
    "\n",
    "The main issue with message aggregation is that information from node v itself could get lost. Computation of $h_{v}^{(l)}$ does not directly depend on $h_{v}^{(l-1)}$. The soltion to this is to include $h_{v}^{(l-1)}$ when computing $h_{v}^{(l)}$. With message, we compute message from node v itself. With aggregation, we look after aggregating from neighbors and we aggregate the message from node v itself via concatenation or summation.\n",
    "\n",
    "Nonlinearity (activation) functions here add expressiveness and can be added to message or aggregation.\n",
    "\n",
    "### Graph Convolutional Networks\n",
    "\n",
    "Graph convolutional networks (GCN):\n",
    "\n",
    "$h_{v}^{(l)}=\\sigma (w^{l} \\sum_{u \\in N(v)}) \\frac{h_{u}^{l-1}}{|N(v)|} $\n",
    "\n",
    "How do we write this as Message + Aggregation?\n",
    "\n",
    "$h_{v}^{(l)}=\\sigma (\\sum_{u \\in N(v)}) \\frac{h_{u}^{l-1}}{|N(v)|} $\n",
    "\n",
    "The aggregation part is: $\\sum_{u \\in N(v)}$\n",
    "\n",
    "The message part is: $W^{(l)}\\frac{h_{u}^{l-1}}{|N(v)|} $\n",
    "\n",
    "\n",
    "### GraphSAGE\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "156b6e445808b07dc40aa5a8e66ccf91c9fbe531dfe1445bc12a4896e0fba728"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
