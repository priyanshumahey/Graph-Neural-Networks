{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning about Graph Neural Networks\n",
    "These lectures are made in conjuction with CS224W resources from Standford as well as the Graph Representation Learning book by William L. Hamilton.\n",
    "\n",
    "This module will go over and explain how graph neural networks work from the ground up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs are common data structures and work as a universal language for describing complex systems. Graphs are described as collections of objects that have interactions (or edges) between them. \n",
    "\n",
    "![Graph](./Images/graph.png)\n",
    "\n",
    "Graphs are able to establish relationships between points and are highly generizable. There are a lot different types of data that can be represented as a graph. Another huge benefit of graphs is they have a mathematical foundation which can be used to analyze and understand them. We can make use of them to make complex predictions. \n",
    "\n",
    "This is especially important for deep learning as modern deep learning is often specialized for simple, linear sequences and grids. \n",
    "\n",
    "Graphs and networks are much more complex to process. They possess arbitrary sizes and complex topological structure (i.e., no spatial locality like grids). They have no fixed node ordering and also lack reference points. Pair that with dynamic and multimodal features, they become really complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs in Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A graph $G = (V, E)$ is described as having a set of nodes $V$ and a set of edges $E$ between the nodes. An edge going from node $u ∈ V$ to node $v ∈ V$ is denoted as $(u, v) ∈ E$. \n",
    "\n",
    "- **Simple Graphs**: These are graphs where there is at most one edge between each pair of nodes, no edges between a node and itself and all edges have no direction.\n",
    "\n",
    "One potential way to represent graphs is through adjacency matrixes: $A ∈ R^{|V|×|V|}$. Here, we would order the nodes in the graph such that every node indexes a particualr row and column in the adjacency matrix. Then, we represent the presence of edges as entries in the matrix: $\\textbf{A}[u,v] = 1$ if $(u,v) ∈ E$ and $\\textbf{A}[u,v] = 0$ otherwise. If a graph contains no directed edges, the graph will be symmetric but if it does have direction, then A may not be symmetric. There is also the chance that graphs have weighted edges where the entries in the adjacency matrix are arbitrary real-values rather than {0,1}.\n",
    "\n",
    "![Graph](./Images/admat.png)\n",
    "\n",
    "Since graphs may often be sparse, we can also represent graphs using an adjacency list. **Adjacency lists** are easier to work with if the network is large and spare and it allows us to quickly retrieve all neighbors of a given node. For example, for the directed graph above, we can create an adjacency list as such:\n",
    "\n",
    "1: 4\n",
    "\n",
    "2: 1\n",
    "\n",
    "3:\n",
    "\n",
    "4: 2, 3\n",
    "\n",
    "It is also possible for graphs to have different kinds of edges beyond undirected, weighted and directed edges. We can expand edge notation to include an edge or relation type τ, e.g., $(u, τ, v) ∈ E$ and we can define one adjacency matrix $\\textbf{A}_{τ}$ per edge type. We call these **mutli-relational** graphs and this entire graph can be summarized using the adjacency tensor $A ∈ R^{|V|×|R|×|V|}$ where $R$ is the set of relations. There are two important subsets of mutli-relationship graphs:\n",
    "\n",
    "1. **Heterogeneous Graphs**: Heterogenous graphs have nodes with imbued types. They contain either multiple types of objects or multiple types of links. For these sorts of graphs, edges typically have to follow some sort of constraint. The most common is what specific types of objects can attach to. \n",
    "2. **Mutliplex Graphs**: Multiplex graphs assume that graphs can be made into a set of $k$ layers. Nodes are replicated accross layers but each layer has differing connectivity. Inter-layer edge types can exist to connect same nodes across layers. \n",
    "\n",
    "We also have attributes or features associated with a graph. These can be represented with a real-valued matrix $X ∈ R^{|V |×m}$ where the ordering is assumed to be the same as the ordering in the adjacency matrix. \n",
    "\n",
    "Node degrees are also another important concept to understand for undirected graphs. A **node degree, $k_{i}$** is the number of edges adjacent to Node $i$. For example, \n",
    "\n",
    "![Graph](./Images/ND4.png)\n",
    "\n",
    "$k_{A}=4$\n",
    "\n",
    "**Avg. Degree:** = $\\bar{k}=\\langle k \\rangle = \\frac{1}{N}\\sum_{N}^{i=1}{k_{i}}=\\frac{2E}{N}$\n",
    "\n",
    "For directed networks, we define an **in-degree** and an **out-degree**. The (total) degree of a node is the sum of in- and out- degrees.\n",
    "\n",
    "![Graph](./Images/NDDirected.png)\n",
    "\n",
    "$k_{C}^{in} = 2$ and $k_{C}^{out}=1$ and thus $k_{C}=3$\n",
    "\n",
    "$\\bar{k} = \\frac{E}{N}$ and here $\\bar{k^{in}} = \\bar{k^{out}}$\n",
    "\n",
    "**Bipartite graphs** are graphs whose nodes can be divided into two disjoint sets $U$ and $V$ such that every link connects a node in $U$ to one in $V$; that is, $U$ and $V$ are **independent sets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning On Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning with graphs, we don't necesarily rely on the typical supervised and unsupervised systems to provide us information about the graphs. We'll start off by discussing what methods would be helpful for machine learning on graphs and discuss their implementation. With deep learning on graphs, our main goal is to input a network and then have an output of predictions which could be node labels, new links, generated graphs and subgraphs, etc, etc.\n",
    "\n",
    "![Graph](./Images/GNNDL.png)\n",
    "\n",
    "In traditional machine learning, a lot of feature engineering is required but with representation learning, the network is able to automatically learn the features. We can think of representation learning as a way to map nodes of a graph to d-dimensional embeddings such that similar nodes in the network are embedded close together.\n",
    "\n",
    "\n",
    "**Node Classification** is when the goal is to predict the label $y_{u}$ which could be a type, category, or attribute associated with all the notes $u ∈ V$ when we only given the true labels on a training set of nodes $V_{train}∈ V$. This is the most popular machine learning task done on graph data. Node classification may appear to be somewhat similar to standard supervised classification, but there are many important differences to consider. The most important thing to consider is that the nodes on a graph are not independent and identically distributed. With supervised learning, we assume the datapoints are statistically independent from all other datapoints and that they are identically distributed. For graphs, its a good idea to leverage their connectedness through ideas such as **homophily**, **structural equivalence** and **heterophily**.\n",
    "- Homophily: This is the tendency for nodes to share atteributes with their neighbors in the graph. \n",
    "- Structural equivalence: This is the idea that nodes with similar local neighborhood structures will have similar lables.\n",
    "- Heterophily: Presumes that nodes have preferentially connected to nodes with different labels. \n",
    "\n",
    "**Link Prediction** or **Relation Prediction** is a method to predict whether there are missing links between two nodes. The standard set up for this is that \n",
    "\n",
    "**Graph Classification**: Categorize different graphs\n",
    "\n",
    "**Clustering:** Detecting if nodes are forming an interconnected community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should pause and take a look at `Notebook 1.ipynb` in the Learning folder to get a better sense of how to program graphs using NetworkX. For more information, take a look at the [documentation](https://networkx.org/documentation/stable/).\n",
    "\n",
    "This will also go over a bit of PyTorch Geometric (PyG) which is helpful for developing Graph Deep Learning models. A lot of this stuff will be somewhat new but it'll make much more sense as time goes on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Methods for Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Some really common methods often applied to graphs include node-level predictions, link-level predictions, graph-level predictions. The traditional machine learning pipeline includes designing features for nodes/links/graphs and then obtaining features for all the training data.\n",
    "\n",
    "### Traditional ML Pipeline\n",
    "The traditional machine learning pipeline is all about designing proper features. \n",
    "\n",
    "One of the earliest steps is designing features for ndoes/links/graphs and obtaining features for all training data. \n",
    "\n",
    "From there, we could train a machine learning model (ie, random forest, SVM, neural network) and from there, we would apply the model to be given a new node/link/graph and obtain its features and make a prediction.\n",
    "\n",
    "Using effective features over graphs is key to achieving good model performance. Traditional ML pipelines use hand-designed features.\n",
    "\n",
    "For node classification, it is imperative that the nodes have well defined features as ML needs features. If we want to characterize the structure and position of a node in the network, some important things to know is node degree, node centrality, clustering coefficent and graphlets.\n",
    "\n",
    "Let's go over a couple of basic things important for applying traditional methods to graphs:\n",
    "\n",
    "**Node degree**: Most obvious straightforward node feature is degree, denoted as $d_{u}$ for a node u is an element of set V and simply counts the number of edges of incident to a node. Measures how many neighbors a node has but this doesn't necessarily measure the importance of the node. \n",
    "\n",
    "$d_{u}=\\sum_{v∈V}{A[u,v]}$\n",
    "\n",
    "**Node centrality**: Takes into account the importance of the node in the graph. An important measure of centrality is the so-called *eigenvector centrality* which, unlike degree that measures amount of neighbors, eigenvector centrality measures how important a node's neighbors are. A node's eigenvector centrality $e_{u}$ via a recurrence relation in which the node's centrality is proportional to the average centrality of its neighbors:\n",
    "\n",
    "$e_{u}=\\frac{1}{\\lambda}\\sum{[u,v]e_{v}∀u ∈ V}$\n",
    "\n",
    "where $\\lambda$ is constant and represents the normalization constant (it will turn out to be the largest eigenvalue of A). When written in vector notation with e as the vector of node centralities, we can see that this recurrence defines the standard eigenvector equation for the adjacency matrix:\n",
    "\n",
    "$\\lambda e = Ae$\n",
    "\n",
    "This means that centrality measure that satisfies the recurrence in the first node centrality equation corresponds to an eigenvector of the adjacency matrix. One view of eigenvector centrality is that it ranks the likelihood that a node is visited on a random walk of inifinite length on the grpah.\n",
    "\n",
    "**Clustering coefficient**: Measures how connected v's neighboring nodes are:\n",
    "\n",
    "$\\frac { number \\hspace{0.1cm} of \\hspace{0.1cm} (edges \\hspace{0.1cm} among  \\hspace{0.1cm} neighboring \\hspace{0.1cm} nodes)}{{k_{v} \\choose 2} } ∈ [0,1]$\n",
    "\n",
    "![Clustering coefficient Example](./Images/ClusteringCo.png)\n",
    "\n",
    "So as a short summary of node featrues:\n",
    "\n",
    "They can be categorized as:\n",
    "- Importance based features (capture importance of a node in a graph):\n",
    "    - Node degree (simply counts the number of neighboring nodes)\n",
    "    - Different node centrality measures (models improtance of neighboring nodes in a graph)\n",
    "- Structure based features:\n",
    "    - Node degree (counts number of neighboring nodes)\n",
    "    - Clustering coefficient (measures how connected neighboring nodes are)\n",
    "    - Graphlet count vector (counts the occurences of different graphlets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link-Level Tasks and Features\n",
    "\n",
    "For now, let's work on the task to predict new links based on existing links. The key here is to design features for a pair of nodes. For link prediction as our task, there are two important formulations:\n",
    "1. Links missing at random:\n",
    "    - Remove a random set of links and then aim to predict them\n",
    "    - More useful for static networks such as protein-protein interaction networks\n",
    "2. Links over time:\n",
    "    - Given $G[t_{0}, t'_{0}]$ a graph on edges up to time $t'_{0}$ output a ranked list L of links (not in $G[t_{0}, t'_{0}]$) that are predicted to appear in $G[t_{1}, t'_{1}]$\n",
    "    - Useful for networks that evolve over time\n",
    "    - **Evaluation**: $n=|E_{new}|$: # of new edges that appear during the test period $[t_{1}, t'_{1}]$\n",
    "    - Take top *n* elements of *L* and count correct edges\n",
    "\n",
    "Let's think about how we could provide a feature descriptor for a pair of given nodes.\n",
    "For a pair of nodes (x,y), we can compute a score c(x,y). This, for example, could be the # of common neighbors of x and y. From there, we could sort pairs (x,y) by the decreasing score c(x,y). Then, we could predict top n pairs as new  links. We then see which of these links actually appears in $G[t_{1}, t'_{1}]$\n",
    "\n",
    "There are a couple of important features we should discuss here:\n",
    "- Distance-based feature\n",
    "    - We could try to capture how far away the nodes are from each other but this does not capture the degree of neighborhood overlap. Node pair (B, H) has 2 shared neighboring nodes while pairs (B, E) and (A, B) only have 1 such node.\n",
    "![Distance-based feature Example](./Images/DistanceBFEx1.png)\n",
    "- Local neighborhood overlap\n",
    "    - Captures # of neighboring nodes shared between two nodes $v_{1}$ and $v_{2}$:\n",
    "    - Common neighbors: $|N(v_{1}∩N(v_{2}))|$\n",
    "        - For the example below: $|N(A)∩N(B)| = |{C}| = 1$\n",
    "    - Jaccard's coefficient: $\\frac{|N(v_{1}∩N(v_{2}))|}{|N(v_{1}∪N(v_{2}))|}$\n",
    "        - For example: $\\frac{|N(A)∩N(B)|}{|N(A)∪N(B)|} = \\frac{|{C}|}{|{C,D}} = \\frac{1}{2}$\n",
    "    - Adamic-Adar index: $\\sum_{u∈N(v_{1})∩N(v_{2})}\\frac{1}{log(k_u)}$\n",
    "        - For example: $\\frac{1}{log(k_{C})}=\\frac{1}{log4}$\n",
    "        ![Local neighborhood overlap Example](./Images/LNO.png) \n",
    "\n",
    "    - There are a couple limitations of local neighboorhood features. These include metric is always zero if the two nodes do not have any neighbors in common. The two nodes, however, may still potentially be connected in the future. Global neighborhood overlap metrics can resolve some of these limitations.\n",
    "\n",
    "- Global neighborhood overlap\n",
    "    - Global neighboorhood metrics focuses on and considers the entire graph.\n",
    "    - **Katz index**: Count the number of walks of all lengths between a given pair of nodes. Computed using the powers of the graph adjacency matrix. \n",
    "        - Computing # of walks between two nodes:\n",
    "            - Recall: $A_{uv}$ if $u ∈ N(v)$\n",
    "            - Let $P^{(K)}_{uv}$ = # of walks of length K between u and v\n",
    "            - We will show $P^{K} = A^{k}$\n",
    "            - $P^{(1)}_{uv}$ = # of walks of length 1 (direct neighborhood) between u and v=$A_{uv}$\n",
    "            - ![Global neighborhood overlap Example](./Images/glo.png)\n",
    "\n",
    "        - How do you compute $P^{2}_{uv}$\n",
    "            - Step 1: compute # of walks of length 1 between each of u's neighbor and v\n",
    "            - Step 2: Sum up these # of walks across u's neighbors\n",
    "            - $P^{2}_{uv} = \\sum_{i}{A_{ui}}*P_{iv}^{(1)}=\\sum_{i}{A_{ui}}*A_{iv}=A^2_{uv}$\n",
    "            - ![Global neighborhood overlap Example 2](./Images/glo2.png)\n",
    "    - Katz index between $v_{1}$ and $v_{2}$ is calculated as the sum over all walk lengths\n",
    "        - $S_{v_{1}v_{2}}=\\sum^{∞}_{l=1}\\beta^{l}A^{l}_{v_{1}v_{2}}$\n",
    "        - $A^{l}_{v_{1}v_{2}}$ represents the # of walks of length l between $v_{1}$ and $v_{2}$.\n",
    "        - $0 < \\beta 1$: discount factor\n",
    "    - The index matrix is computed in closed-form:\n",
    "        - $ s = \\sum^{∞}_{i=1}\\beta^{i}A^{i}$ = $ (I-\\beta A)^{-1}-I$\n",
    "        - They are equal by geometric sum of matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph-Level Features and Graph Kernals\n",
    "**Kernal methods** are widely used for traditional ML for graph-level prediction. Instead of feature vectors, the idea is to use kernals.\n",
    "Quick intro to kernals:\n",
    "- Kernal $K(G,G') ∈ ℝ$ measures similarity between data\n",
    "- Kernal matrix $ K = (K (G,G'))_{G,G'}$ must always be positive semidefinite (i.e., has positive eigenvalues)\n",
    "- There exists a feature representation $\\phi (∙)$ such that $K(G,G') = \\phi (G)^{T}\\phi (G')$\n",
    "- Once the kernal is defined, off the top shelf modles such as kernal SVM can be used to make predictions\n",
    "We will be discussing mainly graphlet kernals and Weisfeiler-Lehman kernals.\n",
    "\n",
    "Graphlet kernals:\n",
    "- Given two graphs G and G', graplet kernals are computed as:\n",
    "    - $K(G,G')=f_{G}^{T}f_{G'}$\n",
    "The problem is that if G and G' have different sizes, that will end up greatly skewing the value so we need normalize each featrue vector.\n",
    "- $h_{G}=\\frac{f_{G}}{Sum(f_{G})}$ and $K(G,G')=h_{G}^{T}h_{G'}$\n",
    "The major limitations come from the fact that counting graphlets are expensive and counting size-k graplets for a graph with size n by enumeration takes $n^{k}$. \n",
    "\n",
    "Weisfeiler-Lehman Kernal\n",
    "- This is a computationally efficient algorithm that usess color refinement and time complexity becomes much more linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Embedding\n",
    "\n",
    "Node embeddings is an important method to learn as it lets us encode nodes as low-dimensional vecrtors that can summarize their graph position and the structure of their local graph neighborhood. We want to make the nodes to a latent space where geometric relations in this latent space correspond to relationships in the original graph or network. \n",
    "\n",
    "![Node embedding](./Images/nodemed.png)\n",
    "\n",
    "Node embedding is an important part of graph representation learning (automatically learning the features) as it alleviates the need to do feature engineering every single time.\n",
    "\n",
    "When we map nodes into an embedding space, we focus on mapping the similarity of embeddings between nodes to indicate their similarity in the network. It also encodes important network information and is potentially used for many downstream predictions.\n",
    "\n",
    "### Encoder Decoder Prespective\n",
    "The encoder-decoder framework allows us to view the graph representation learning problem as involving two key operations. First is the encoder model that maps each node in the graph into a low-dimensional vector or embedding. Next, a decoder model will take the low-dimensional noode embeddings and use them to reconstruct information about each node's neighborhood in the original graph. \n",
    "\n",
    "![Encoder decoder](./Images/encodedecode.png)\n",
    "\n",
    "The encoder will map from nodes to embeddings and then we can define a node similarity function. The decoder will map from embeddings to the similarity score and from there we can optimize the parameters of the encoder such that the similarity in the original network will roughly equal similarity of the embedding.\n",
    "\n",
    "#### Encoder\n",
    "The encoder maps nodes $v ∈ V$ to vector embeddings $z_{v} ∈ R^{d}$ (where $z_{v}$ corresponds to the embeddings for node $v∈ V$). In the simplest case, the encoder takes node IDs as input to generate the nodef embeddings. In most work on node embeddings, the encoder relies on what we call the shallow embedding approach, where this encoder function is simply embedding lookup based on the node ID. In other words, we have that:\n",
    "- ENC(v) = Z[v]\n",
    "where $Z ∈ R^{|V|×d}$ is a matrix containing the embedding vectors for all nodes and Z[v] denotes the row of Z corresponding to node v.\n",
    "\n",
    "#### Decoder\n",
    "The decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
